trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    default: nonprod
    values: [nonprod, prod]

  - name: destroy
    displayName: Destroy
    type: boolean
    default: false

variables:
  TF_DIR: terraform
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'
  AWS_REGION: us-east-1
  CLUSTER_NAME: 'nonprod-eks-cluster'
  HELM_NAMESPACE: 'kube-system'
  SERVICE_ACCOUNT: 'aws-load-balancer-controller'
  HELM_RELEASE: 'aws-load-balancer-controller'
  APP_DIR: k8s/hello-manar
  AWS_SERVICE_CONNECTION: manar

stages:

# =========================
# 1️⃣ Terraform Stage
# =========================
- stage: Infra
  displayName: Terraform Infra
  jobs:
    - job: Terraform
      pool:
        vmImage: ubuntu-latest
      steps:
        - checkout: self

        - task: TerraformInstaller@1
          displayName: Install Terraform
          inputs:
            terraformVersion: latest

        - task: AWSShellScript@1
          displayName: "Check AWS Credentials"
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              aws sts get-caller-identity

        - task: AWSShellScript@1
          displayName: Terraform init
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              cd "$(TF_DIR)"
              terraform init -input=false

        - task: AWSShellScript@1
          displayName: Terraform plan
          condition: eq('${{ parameters.destroy }}', false)
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              cd "$(TF_DIR)"
              if [[ ! -f "$(TF_VAR_FILE)" ]]; then
                echo "❌ Variables file '$(TF_VAR_FILE)' not found"
                exit 1
              fi
              terraform plan -out=tfplan -input=false -var-file="$(TF_VAR_FILE)"

        - task: AWSShellScript@1
          displayName: Terraform plan -destroy
          condition: eq('${{ parameters.destroy }}', true)
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              cd "$(TF_DIR)"
              if [[ ! -f "$(TF_VAR_FILE)" ]]; then
                echo "❌ Variables file '$(TF_VAR_FILE)' not found"
                exit 1
              fi
              terraform plan -destroy -out=tfplan -input=false -var-file="$(TF_VAR_FILE)"

        - task: AWSShellScript@1
          displayName: Terraform apply
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              cd "$(TF_DIR)"
              if [[ ! -f "tfplan" ]]; then
                echo "❌ Terraform plan 'tfplan' not found"
                exit 1
              fi
              terraform apply -auto-approve -input=false tfplan

# =========================
# 2️⃣ ALB Helm Stage
# =========================
- stage: ALB_Controller
  dependsOn: Infra
  displayName: Install ALB Controller
  jobs:
    - job: Helm_ALB
      pool:
        vmImage: ubuntu-latest
      steps:
        - checkout: self

        - task: KubectlInstaller@0
          inputs:
            versionSpec: 'latest'

        - task: HelmInstaller@1
          inputs:
            helmVersionToInstall: 'latest'

        # ✅ Configure kubeconfig using Service Connection (without exec plugin)
        - task: AWSShellScript@1
          displayName: Configure kubeconfig
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              set -euo pipefail
              
              # export AWS keys from Service Connection
              export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
              export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
              export AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN

              # update kubeconfig for this pipeline
              aws eks update-kubeconfig --name $(CLUSTER_NAME) --region $(AWS_REGION) --alias $(CLUSTER_NAME) --kubeconfig kubeconfig.yaml

        # ✅ Create Service Account (IRSA)
        - task: AWSShellScript@1
          displayName: "Create Service Account (IRSA)"
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              set -euo pipefail
              export KUBECONFIG=kubeconfig.yaml
              
              kubectl create namespace $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -
              kubectl create serviceaccount $(SERVICE_ACCOUNT) -n $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -
              ROLE_ARN=$(aws iam get-role --role-name $(CLUSTER_NAME)-alb-controller-role --query Role.Arn --output text)
              kubectl annotate serviceaccount $(SERVICE_ACCOUNT) \
                eks.amazonaws.com/role-arn=$ROLE_ARN \
                -n $(HELM_NAMESPACE) --overwrite

        # ✅ Install ALB Controller using Helm
        - task: AWSShellScript@1
          displayName: "Install ALB Controller"
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              set -euo pipefail
              export KUBECONFIG=kubeconfig.yaml

              helm repo add eks https://aws.github.io/eks-charts
              helm repo update
              helm upgrade --install $(HELM_RELEASE) eks/aws-load-balancer-controller \
                -n $(HELM_NAMESPACE) \
                --set clusterName=$(CLUSTER_NAME) \
                --set serviceAccount.create=false \
                --set serviceAccount.name=$(SERVICE_ACCOUNT)

# =========================
# 3️⃣ Deploy App Stage
# =========================
- stage: Deploy_App
  dependsOn: ALB_Controller
  displayName: Deploy Hello From Manar
  jobs:
    - job: Deploy_App
      pool:
        vmImage: ubuntu-latest
      steps:
        - checkout: self

        - task: KubectlInstaller@0
          inputs:
            versionSpec: 'latest'

        # Configure kubeconfig + export AWS keys
        - task: AWSShellScript@1
          displayName: Configure kubeconfig
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              set -euo pipefail
              export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
              export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
              export AWS_SESSION_TOKEN=$AWS_SESSION_TOKEN
              aws eks update-kubeconfig --name $(CLUSTER_NAME) --region $(AWS_REGION) --alias $(CLUSTER_NAME) --kubeconfig kubeconfig.yaml

        # Deploy App + Ingress
        - task: AWSShellScript@1
          displayName: "Deploy App + Ingress"
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              set -euo pipefail
              export KUBECONFIG=kubeconfig.yaml
              kubectl apply -f $(APP_DIR)/
