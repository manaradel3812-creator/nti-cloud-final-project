trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    default: nonprod
    values: [nonprod, prod]

  - name: destroy
    displayName: Destroy
    type: boolean
    default: false

variables:
  TF_DIR: terraform
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'
  AWS_REGION: us-east-1
  CLUSTER_NAME: 'nonprod-eks-cluster'
  HELM_NAMESPACE: 'kube-system'
  SERVICE_ACCOUNT: 'aws-load-balancer-controller'
  HELM_RELEASE: 'aws-load-balancer-controller'
  APP_DIR: k8s/hello-manar

stages:

# =========================
# 1️⃣ Terraform Stage
# =========================
- stage: Infra
  displayName: Terraform Infra
  jobs:
    - job: Terraform
      pool:
        vmImage: ubuntu-latest
      steps:
        - checkout: self

        # Install Terraform
        - task: TerraformInstaller@1
          displayName: Install Terraform
          inputs:
            terraformVersion: latest

        # Check AWS identity using Service Connection
        - task: AWSShellScript@1
          displayName: "Check AWS Credentials"
          inputs:
            awsCredentials: 'manar'
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              aws sts get-caller-identity

        # Terraform init
        - task: AWSShellScript@1
          displayName: Terraform init
          inputs:
            awsCredentials: 'manar'
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              cd "$(TF_DIR)"
              terraform init -input=false

        # Terraform plan (normal)
        - task: AWSShellScript@1
          displayName: Terraform plan
          condition: eq('${{ parameters.destroy }}', false)
          inputs:
            awsCredentials: 'manar'
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              cd "$(TF_DIR)"
              if [[ ! -f "$(TF_VAR_FILE)" ]]; then
                echo "❌ Variables file '$(TF_VAR_FILE)' not found"
                exit 1
              fi
              terraform plan -out=tfplan -input=false -var-file="$(TF_VAR_FILE)"

        # Terraform plan (destroy)
        - task: AWSShellScript@1
          displayName: Terraform plan -destroy
          condition: eq('${{ parameters.destroy }}', true)
          inputs:
            awsCredentials: 'manar'
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              cd "$(TF_DIR)"
              if [[ ! -f "$(TF_VAR_FILE)" ]]; then
                echo "❌ Variables file '$(TF_VAR_FILE)' not found"
                exit 1
              fi
              terraform plan -destroy -out=tfplan -input=false -var-file="$(TF_VAR_FILE)"

        # Terraform apply
        - task: AWSShellScript@1
          displayName: Terraform apply
          inputs:
            awsCredentials: 'manar'
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              cd "$(TF_DIR)"
              if [[ ! -f "tfplan" ]]; then
                echo "❌ Terraform plan 'tfplan' not found"
                exit 1
              fi
              terraform apply -auto-approve -input=false tfplan

# =========================
# 2️⃣ ALB Helm Stage
# =========================
- stage: ALB_Controller
  dependsOn: Infra
  displayName: Install ALB Controller
  jobs:
    - job: Helm_ALB
      pool:
        vmImage: ubuntu-latest
      steps:
        - checkout: self

        # Install Kubectl & Helm
        - task: KubectlInstaller@0
          inputs:
            versionSpec: 'latest'
        - task: HelmInstaller@1
          inputs:
            helmVersionToInstall: 'latest'

        # Configure kubeconfig using exec block
        - task: AWSShellScript@1
          displayName: Configure kubeconfig (exec block)
          inputs:
            awsCredentials: 'manar'
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              set -euo pipefail

              CLUSTER_NAME=$(CLUSTER_NAME)
              AWS_REGION=$(AWS_REGION)

              ENDPOINT=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION --query "cluster.endpoint" --output text)
              CERT_DATA=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION --query "cluster.certificateAuthority.data" --output text)

              mkdir -p $HOME/.kube
              cat > $HOME/.kube/config <<EOF
apiVersion: v1
clusters:
- cluster:
    server: $ENDPOINT
    certificate-authority-data: $CERT_DATA
  name: $CLUSTER_NAME
contexts:
- context:
    cluster: $CLUSTER_NAME
    user: $CLUSTER_NAME
  name: $CLUSTER_NAME
current-context: $CLUSTER_NAME
kind: Config
preferences: {}
users:
- name: $CLUSTER_NAME
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      command: aws
      args:
        - "eks"
        - "get-token"
        - "--cluster-name"
        - "$CLUSTER_NAME"
        - "--region"
        - "$AWS_REGION"
EOF

        # Create Service Account (IRSA)
        - task: AWSShellScript@1
          displayName: "Create Service Account (IRSA)"
          inputs:
            awsCredentials: 'manar'
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              kubectl create namespace $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -
              kubectl create serviceaccount $(SERVICE_ACCOUNT) -n $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -
              kubectl annotate serviceaccount $(SERVICE_ACCOUNT) \
                eks.amazonaws.com/role-arn=$(aws iam get-role --role-name $(CLUSTER_NAME)-alb-controller-role --query Role.Arn --output text) \
                -n $(HELM_NAMESPACE) --overwrite

        # Install ALB Controller using Helm
        - task: AWSShellScript@1
          displayName: "Install ALB Controller"
          inputs:
            awsCredentials: 'manar'
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              helm repo add eks https://aws.github.io/eks-charts
              helm repo update
              helm upgrade --install $(HELM_RELEASE) eks/aws-load-balancer-controller \
                -n $(HELM_NAMESPACE) \
                --set clusterName=$(CLUSTER_NAME) \
                --set serviceAccount.create=false \
                --set serviceAccount.name=$(SERVICE_ACCOUNT)

# =========================
# 3️⃣ Deploy App Stage
# =========================
- stage: Deploy_App
  dependsOn: ALB_Controller
  displayName: Deploy Hello From Manar
  jobs:
    - job: Deploy_App
      pool:
        vmImage: ubuntu-latest
      steps:
        - checkout: self

        - task: KubectlInstaller@0
          inputs:
            versionSpec: 'latest'

        # Configure kubeconfig using exec block
        - task: AWSShellScript@1
          displayName: Configure kubeconfig (exec block)
          inputs:
            awsCredentials: 'manar'
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              set -euo pipefail

              CLUSTER_NAME=$(CLUSTER_NAME)
              AWS_REGION=$(AWS_REGION)

              ENDPOINT=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION --query "cluster.endpoint" --output text)
              CERT_DATA=$(aws eks describe-cluster --name $CLUSTER_NAME --region $AWS_REGION --query "cluster.certificateAuthority.data" --output text)

              mkdir -p $HOME/.kube
              cat > $HOME/.kube/config <<EOF
apiVersion: v1
clusters:
- cluster:
    server: $ENDPOINT
    certificate-authority-data: $CERT_DATA
  name: $CLUSTER_NAME
contexts:
- context:
    cluster: $CLUSTER_NAME
    user: $CLUSTER_NAME
  name: $CLUSTER_NAME
current-context: $CLUSTER_NAME
kind: Config
preferences: {}
users:
- name: $CLUSTER_NAME
  user:
    exec:
      apiVersion: client.authentication.k8s.io/v1alpha1
      command: aws
      args:
        - "eks"
        - "get-token"
        - "--cluster-name"
        - "$CLUSTER_NAME"
        - "--region"
        - "$AWS_REGION"
EOF

        # Deploy App + Ingress
        - task: AWSShellScript@1
          displayName: "Deploy App + Ingress"
          inputs:
            awsCredentials: 'manar'
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              kubectl apply -f $(APP_DIR)/
