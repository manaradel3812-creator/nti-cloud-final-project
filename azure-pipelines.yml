trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    default: nonprod
    values: [nonprod, prod]
  - name: destroy
    displayName: Destroy
    type: boolean
    default: false

variables:
  TF_DIR: terraform
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'
  AWS_REGION: us-east-1
  CLUSTER_NAME: 'nonprod-eks-cluster'
  HELM_NAMESPACE: 'kube-system'
  SERVICE_ACCOUNT: 'aws-load-balancer-controller'
  HELM_RELEASE: 'aws-load-balancer-controller'
  APP_DIR: k8s/hello-manar
  AWS_SERVICE_CONNECTION: manar
  # Ù…Ø³Ø§Ø± Ù…ÙˆØ­Ø¯ Ù„Ù…Ù„Ù Ø§Ù„ÙƒÙˆØ¨Ù†ØªÙŠØ² Ù„Ø¶Ù…Ø§Ù† Ø§Ù„ÙˆØµÙˆÙ„ Ø¥Ù„ÙŠÙ‡ ÙÙŠ ÙƒÙ„ Task
  KUBECONFIG_PATH: $(Pipeline.Workspace)/kubeconfig.yaml

stages:

# =========================
# 1ï¸âƒ£ Terraform Stage
# =========================
- stage: Infra
  displayName: Terraform Infra
  jobs:
    - job: Terraform
      pool:
        vmImage: ubuntu-latest
      steps:
        - checkout: self
        - task: TerraformInstaller@1
          displayName: Install Terraform
          inputs:
            terraformVersion: latest

        - task: AWSShellScript@1
          displayName: "Terraform Run"
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              set -e
              cd "$(TF_DIR)"
              terraform init -input=false
              
              if [ "${{ parameters.destroy }}" == "true" ]; then
                echo "ğŸ”¥ Destroying Infrastructure..."
                terraform destroy -auto-approve -input=false -var-file="$(TF_VAR_FILE)"
              else
                echo "ğŸš€ Applying Infrastructure..."
                terraform apply -auto-approve -input=false -var-file="$(TF_VAR_FILE)"
              fi

# =========================
# 2ï¸âƒ£ ALB Helm Stage
# =========================
- stage: ALB_Controller
  dependsOn: Infra
  condition: and(succeeded(), eq('${{ parameters.destroy }}', false))
  displayName: Install ALB Controller
  jobs:
    - job: Helm_ALB
      pool:
        vmImage: ubuntu-latest
      steps:
        - checkout: self
        - task: KubectlInstaller@0
          inputs: { versionSpec: 'latest' }
        - task: HelmInstaller@1
          inputs: { helmVersionToInstall: 'latest' }

        - task: AWSShellScript@1
          displayName: "Prepare Kubeconfig & IRSA"
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              set -euo pipefail
              
              # ØªØ­Ø¯ÙŠØ« Ù…Ù„Ù Ø§Ù„ÙƒÙˆØ¨Ù†ØªÙŠØ² ÙÙŠ Ù…Ø³Ø§Ø± Ø«Ø§Ø¨Øª
              aws eks update-kubeconfig --name $(CLUSTER_NAME) --region $(AWS_REGION) --kubeconfig $(KUBECONFIG_PATH)
              export KUBECONFIG=$(KUBECONFIG_PATH)

              # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù€ Namespace ÙˆØ§Ù„Ù€ Service Account
              kubectl create namespace $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -
              kubectl create serviceaccount $(SERVICE_ACCOUNT) -n $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -
              
              # Ø¬Ù„Ø¨ Ø§Ù„Ù€ Role ARN Ù…Ù† AWS ÙˆØ±Ø¨Ø·Ù‡ Ø¨Ø§Ù„Ù€ Service Account
              ROLE_ARN=$(aws iam get-role --role-name $(CLUSTER_NAME)-alb-controller-role --query Role.Arn --output text)
              kubectl annotate serviceaccount $(SERVICE_ACCOUNT) \
                eks.amazonaws.com/role-arn=$ROLE_ARN \
                -n $(HELM_NAMESPACE) --overwrite

        - task: AWSShellScript@1
          displayName: "Install ALB Controller via Helm"
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              set -euo pipefail
              export KUBECONFIG=$(KUBECONFIG_PATH)

              helm repo add eks https://aws.github.io/eks-charts
              helm repo update
              helm upgrade --install $(HELM_RELEASE) eks/aws-load-balancer-controller \
                -n $(HELM_NAMESPACE) \
                --set clusterName=$(CLUSTER_NAME) \
                --set serviceAccount.create=false \
                --set serviceAccount.name=$(SERVICE_ACCOUNT)

# =========================
# 3ï¸âƒ£ Deploy App Stage
# =========================
- stage: Deploy_App
  dependsOn: ALB_Controller
  condition: and(succeeded(), eq('${{ parameters.destroy }}', false))
  displayName: Deploy Application
  jobs:
    - job: Deploy
      pool:
        vmImage: ubuntu-latest
      steps:
        - checkout: self
        - task: KubectlInstaller@0
          inputs: { versionSpec: 'latest' }

        - task: AWSShellScript@1
          displayName: "Update Kubeconfig & Apply Manifests"
          inputs:
            awsCredentials: $(AWS_SERVICE_CONNECTION)
            regionName: $(AWS_REGION)
            scriptType: inline
            inlineScript: |
              set -euo pipefail
              
              # ØªØ­Ø¯ÙŠØ« Ø§Ù„ØªÙˆØ«ÙŠÙ‚ Ù„Ù„Ù€ Cluster ÙÙŠ Ø§Ù„Ù€ Stage Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©
              aws eks update-kubeconfig --name $(CLUSTER_NAME) --region $(AWS_REGION) --kubeconfig $(KUBECONFIG_PATH)
              export KUBECONFIG=$(KUBECONFIG_PATH)

              # ØªØ·Ø¨ÙŠÙ‚ Ù…Ù„ÙØ§Øª Ø§Ù„Ù€ Kubernetes (Deployment, Service, Ingress)
              if [ -d "$(APP_DIR)" ]; then
                echo "ğŸ“¦ Deploying manifests from $(APP_DIR)..."
                kubectl apply -f $(APP_DIR)/
              else
                echo "âŒ Directory $(APP_DIR) not found!"
                exit 1
              fi