trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    default: nonprod
    values: [nonprod, prod]
  - name: destroy
    displayName: Destroy Infra
    type: boolean
    default: false

variables:
  AWS_REGION: us-east-1
  AWS_SERVICE_CONNECTION: manar
  TF_DIR: terraform
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'
  CLUSTER_NAME: '${{ parameters.env }}-eks-cluster'
  HELM_NAMESPACE: kube-system
  SERVICE_ACCOUNT: aws-load-balancer-controller
  KUBECONFIG_PATH: $(Pipeline.Workspace)/kubeconfig.yaml

stages:

#################################
# 1Ô∏è‚É£ Stage: Terraform Infra
#################################
- stage: Infra
  displayName: Terraform Infra
  jobs:
  - job: Terraform
    pool:
      vmImage: ubuntu-latest
    steps:
    - checkout: self
    - task: TerraformInstaller@1
      inputs: { terraformVersion: latest }

    - task: AWSShellScript@1
      displayName: "Terraform Run"
      inputs:
        awsCredentials: $(AWS_SERVICE_CONNECTION)
        regionName: $(AWS_REGION)
        scriptType: inline
        inlineScript: |
          set -euo pipefail
          cd $(TF_DIR)
          terraform init -input=false
          if [ "${{ parameters.destroy }}" = "true" ]; then
            terraform destroy -auto-approve -var-file=$(TF_VAR_FILE)
          else
            terraform apply -auto-approve -var-file=$(TF_VAR_FILE)
          fi

#################################
# 2Ô∏è‚É£ Stage: Platform / Helm
#################################
- stage: Platform
  displayName: Platform & Helm
  dependsOn: Infra
  condition: and(succeeded(), eq('${{ parameters.destroy }}', false))
  jobs:
  - job: Helm
    pool:
      vmImage: ubuntu-latest
    steps:
    - checkout: self
    - task: KubectlInstaller@0
      inputs: { versionSpec: latest }
    - task: HelmInstaller@1
      inputs: { helmVersionToInstall: latest }

    - task: AWSShellScript@1
      displayName: "Install Platform Tools (ALB, Argo, Cert-Manager)"
      inputs:
        awsCredentials: $(AWS_SERVICE_CONNECTION)
        regionName: $(AWS_REGION)
        scriptType: inline
        inlineScript: |
          set -euo pipefail

          # ‚úÖ ÿ™ÿµÿØŸäÿ± ÿßŸÑŸÖÿ™ÿ∫Ÿäÿ±ÿßÿ™ ŸÑÿ∂ŸÖÿßŸÜ ÿπŸÖŸÑ kubectl
          export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
          export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
          export AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN:-}
          export KUBECONFIG=$(KUBECONFIG_PATH)

          echo "üîê Updating Kubeconfig..."
          aws eks update-kubeconfig --name $(CLUSTER_NAME) --region $(AWS_REGION) --kubeconfig $KUBECONFIG

          echo "üõ†Ô∏è Creating IRSA & Namespace..."
          kubectl create namespace $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -
          kubectl create serviceaccount $(SERVICE_ACCOUNT) -n $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -

          # ŸÖÿ≠ÿßŸàŸÑÿ© ÿ¨ŸÑÿ® ÿßŸÑŸÄ Role ŸÖÿπ Retry (ŸÑÿ£ŸÜŸá ŸÇÿØ Ÿäÿ£ÿÆÿ∞ ŸàŸÇÿ™ÿßŸã ÿ®ÿπÿØ ÿßŸÑŸÄ Terraform)
          ROLE_ARN=""
          for i in {1..6}; do
            ROLE_ARN=$(aws iam get-role --role-name $(CLUSTER_NAME)-alb-controller-role --query Role.Arn --output text 2>/dev/null || echo "")
            if [ -n "$ROLE_ARN" ]; then break; fi
            echo "Waiting for IAM Role... ($i/6)"
            sleep 10
          done

          if [ -z "$ROLE_ARN" ]; then echo "‚ùå Role Not Found"; exit 1; fi

          kubectl annotate serviceaccount $(SERVICE_ACCOUNT) eks.amazonaws.com/role-arn=$ROLE_ARN -n $(HELM_NAMESPACE) --overwrite

          echo "üì¶ Installing Helm Charts..."
          helm repo add eks https://aws.github.io/eks-charts
          helm repo add jetstack https://charts.jetstack.io
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update

          VPC_ID=$(aws eks describe-cluster --name $(CLUSTER_NAME) --query "cluster.resourcesVpcConfig.vpcId" --output text)

          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n $(HELM_NAMESPACE) \
            --set clusterName=$(CLUSTER_NAME) \
            --set region=$(AWS_REGION) \
            --set vpcId=$VPC_ID \
            --set serviceAccount.create=false \
            --set serviceAccount.name=$(SERVICE_ACCOUNT)

          helm upgrade --install cert-manager jetstack/cert-manager -n cert-manager --create-namespace --set installCRDs=true
          helm upgrade --install argocd argo/argo-cd -n argocd --create-namespace