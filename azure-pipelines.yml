trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    default: nonprod
    values: [nonprod, prod]
  - name: destroy
    displayName: Destroy Infra
    type: boolean
    default: false

variables:
  AWS_REGION: us-east-1
  INFRA_CONNECTION: manar    # Ø§Ù„Ù€ Service Connection Ø§Ù„Ø£ÙˆÙ„ Ù„Ù„Ù€ Terraform
  HELM_CONNECTION: manarr     # Ø§Ù„Ù€ Service Connection Ø§Ù„Ø«Ø§Ù†ÙŠ Ù„Ù„Ù€ Helm/App
  TF_DIR: terraform
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'
  CLUSTER_NAME: '${{ parameters.env }}-eks-cluster'
  HELM_NAMESPACE: kube-system
  SERVICE_ACCOUNT: aws-load-balancer-controller
  KUBECONFIG_PATH: $(Pipeline.Workspace)/kubeconfig.yaml

stages:

#################################
# 1ï¸âƒ£ Stage: Terraform Infra (Using manar)
#################################
- stage: Infra
  displayName: "Infrastructure Deployment"
  jobs:
  - job: Terraform
    pool:
      vmImage: ubuntu-latest
    steps:
    - checkout: self
    - task: TerraformInstaller@1
      inputs: { terraformVersion: latest }

    - task: AWSShellScript@1
      displayName: "Run Terraform with manar"
      inputs:
        awsCredentials: $(INFRA_CONNECTION)
        regionName: $(AWS_REGION)
        scriptType: inline
        inlineScript: |
          set -euo pipefail
          cd $(TF_DIR)
          terraform init -input=false
          if [ "${{ parameters.destroy }}" = "true" ]; then
            terraform destroy -auto-approve -var-file=$(TF_VAR_FILE)
          else
            terraform apply -auto-approve -var-file=$(TF_VAR_FILE)
          fi

#################################
# 2ï¸âƒ£ Stage: Platform & App (Using manarr)
#################################
- stage: Platform
  displayName: "Kubernetes Platform Setup"
  dependsOn: Infra
  condition: and(succeeded(), eq('${{ parameters.destroy }}', false))
  jobs:
  - job: Helm
    pool:
      vmImage: ubuntu-latest
    steps:
    - checkout: self
    - task: KubectlInstaller@0
      inputs: { versionSpec: latest }
    - task: HelmInstaller@1
      inputs: { helmVersionToInstall: latest }

    # âœ… Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„Ø£Ù‡Ù…: Ø§Ø³ØªØ®Ø¯Ø§Ù… manarr Ù„Ø¨Ù†Ø§Ø¡ Ù…Ù„Ù Ø§Ù„ØµÙ„Ø§Ø­ÙŠØ§Øª ÙŠØ¯ÙˆÙŠØ§Ù‹
    - task: AWSShellScript@1
      displayName: "Configure AWS Profile with manarr"
      inputs:
        awsCredentials: $(HELM_CONNECTION)
        regionName: $(AWS_REGION)
        scriptType: inline
        inlineScript: |
          set -euo pipefail
          mkdir -p ~/.aws
          echo "[default]" > ~/.aws/credentials
          echo "aws_access_key_id = $AWS_ACCESS_KEY_ID" >> ~/.aws/credentials
          echo "aws_secret_access_key = $AWS_SECRET_ACCESS_KEY" >> ~/.aws/credentials
          if [ -n "${AWS_SESSION_TOKEN:-}" ]; then
            echo "aws_session_token = $AWS_SESSION_TOKEN" >> ~/.aws/credentials
          fi
          
          echo "ğŸ” Updating Kubeconfig..."
          aws eks update-kubeconfig --name $(CLUSTER_NAME) --region $(AWS_REGION) --kubeconfig $(KUBECONFIG_PATH)
          
          echo "âœ… AWS CLI & Kubeconfig are ready."

    # ØªÙ†ÙÙŠØ° Ø£ÙˆØ§Ù…Ø± Kubernetes Ùˆ Helm
    - script: |
        set -euo pipefail
        export KUBECONFIG=$(KUBECONFIG_PATH)
        
        echo "ğŸ” Testing Connection..."
        kubectl get nodes

        echo "ğŸ› ï¸ Creating IRSA & Namespace..."
        kubectl create namespace $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -
        kubectl create serviceaccount $(SERVICE_ACCOUNT) -n $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -

        # Ø¬Ù„Ø¨ Ø§Ù„Ù€ Role ARN Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ù„Ù Ø§Ù„Ù„ÙŠ ÙƒØ±ÙŠØªÙ†Ø§Ù‡ ÙÙˆÙ‚
        ROLE_ARN=$(aws iam get-role --role-name $(CLUSTER_NAME)-alb-controller-role --query Role.Arn --output text)
        
        kubectl annotate serviceaccount $(SERVICE_ACCOUNT) \
          eks.amazonaws.com/role-arn=$ROLE_ARN \
          -n $(HELM_NAMESPACE) --overwrite

        echo "ğŸ“¦ Installing Helm Charts..."
        helm repo add eks https://aws.github.io/eks-charts
        helm repo update
        
        VPC_ID=$(aws eks describe-cluster --name $(CLUSTER_NAME) --query "cluster.resourcesVpcConfig.vpcId" --output text)

        helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
          -n $(HELM_NAMESPACE) \
          --set clusterName=$(CLUSTER_NAME) \
          --set region=$(AWS_REGION) \
          --set vpcId=$VPC_ID \
          --set serviceAccount.create=false \
          --set serviceAccount.name=$(SERVICE_ACCOUNT)

        # ØªØ«Ø¨ÙŠØª Ø§Ù„Ø£Ø¯ÙˆØ§Øª Ø§Ù„Ø¥Ø¶Ø§ÙÙŠØ©
        helm repo add jetstack https://charts.jetstack.io
        helm repo add argo https://argoproj.github.io/argo-helm
        helm repo update
        
        helm upgrade --install cert-manager jetstack/cert-manager -n cert-manager --create-namespace --set installCRDs=true
        helm upgrade --install argocd argo/argo-cd -n argocd --create-namespace
      displayName: "Install Platform Components"