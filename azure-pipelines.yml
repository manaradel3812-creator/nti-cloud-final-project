trigger: none
pr: none

parameters:
  - name: env
    displayName: Environment
    type: string
    default: nonprod
    values: [nonprod, prod]
  - name: destroy
    displayName: Destroy Infra
    type: boolean
    default: false

variables:
  AWS_REGION: us-east-1
  AWS_SERVICE_CONNECTION: manar
  TF_DIR: terraform
  TF_VAR_FILE: '${{ parameters.env }}.tfvars'
  CLUSTER_NAME: '${{ parameters.env }}-eks-cluster'
  HELM_NAMESPACE: kube-system
  SERVICE_ACCOUNT: aws-load-balancer-controller
  KUBECONFIG_PATH: $(Pipeline.Workspace)/kubeconfig.yaml

stages:

#################################
# 1Ô∏è‚É£ Stage: Terraform Infra
#################################
- stage: Infra
  displayName: "Infrastructure Deployment"
  jobs:
  - job: Terraform
    pool:
      vmImage: ubuntu-latest
    steps:
    - checkout: self
    - task: TerraformInstaller@1
      displayName: "Install Terraform"
      inputs: { terraformVersion: latest }

    - task: AWSShellScript@1
      displayName: "Run Terraform"
      inputs:
        awsCredentials: $(AWS_SERVICE_CONNECTION)
        regionName: $(AWS_REGION)
        scriptType: inline
        inlineScript: |
          set -euo pipefail
          cd $(TF_DIR)
          
          terraform init -input=false
          
          if [ "${{ parameters.destroy }}" = "true" ]; then
            echo "üî• Destroying Resources..."
            terraform destroy -auto-approve -var-file=$(TF_VAR_FILE)
          else
            echo "üöÄ Applying Resources..."
            terraform apply -auto-approve -var-file=$(TF_VAR_FILE)
          fi

#################################
# 2Ô∏è‚É£ Stage: Platform / Helm
#################################
- stage: Platform
  displayName: "Kubernetes Platform Setup"
  dependsOn: Infra
  # Only run if Terraform was successful and we are NOT destroying
  condition: and(succeeded(), eq('${{ parameters.destroy }}', false))
  jobs:
  - job: Helm
    pool:
      vmImage: ubuntu-latest
    steps:
    - checkout: self
    - task: KubectlInstaller@0
      inputs: { versionSpec: latest }
    - task: HelmInstaller@1
      inputs: { helmVersionToInstall: latest }

    - task: AWSShellScript@1
      displayName: "Setup EKS Access & Helm Tools"
      inputs:
        awsCredentials: $(AWS_SERVICE_CONNECTION)
        regionName: $(AWS_REGION)
        scriptType: inline
        inlineScript: |
          set -euo pipefail

          # üîë CRITICAL: Export AWS credentials for the kubectl exec-plugin
          # This prevents "Unable to locate credentials" / Exit Code 253
          export AWS_ACCESS_KEY_ID=$AWS_ACCESS_KEY_ID
          export AWS_SECRET_ACCESS_KEY=$AWS_SECRET_ACCESS_KEY
          export AWS_SESSION_TOKEN=${AWS_SESSION_TOKEN:-}
          export AWS_DEFAULT_REGION=$(AWS_REGION)
          export KUBECONFIG=$(KUBECONFIG_PATH)

          echo "üîê Updating Kubeconfig..."
          aws eks update-kubeconfig --name $(CLUSTER_NAME) --region $(AWS_REGION) --kubeconfig $KUBECONFIG

          echo "üîç Verifying Connection..."
          kubectl get nodes

          echo "üõ†Ô∏è Configuring IRSA (IAM Roles for Service Accounts)..."
          kubectl create namespace $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -
          kubectl create serviceaccount $(SERVICE_ACCOUNT) -n $(HELM_NAMESPACE) --dry-run=client -o yaml | kubectl apply -f -

          # Fetch Role ARN with a retry loop (IAM eventually consistency)
          ROLE_ARN=""
          for i in {1..6}; do
            ROLE_ARN=$(aws iam get-role --role-name $(CLUSTER_NAME)-alb-controller-role --query Role.Arn --output text 2>/dev/null || echo "")
            if [ -n "$ROLE_ARN" ]; then break; fi
            echo "Waiting for IAM Role propagation... ($i/6)"
            sleep 10
          done

          if [ -z "$ROLE_ARN" ]; then 
            echo "‚ùå Error: Role $(CLUSTER_NAME)-alb-controller-role not found!"
            exit 1 
          fi

          kubectl annotate serviceaccount $(SERVICE_ACCOUNT) \
            eks.amazonaws.com/role-arn=$ROLE_ARN \
            -n $(HELM_NAMESPACE) --overwrite

          echo "üì¶ Installing Platform Components (ALB, Cert-Manager, ArgoCD)..."
          helm repo add eks https://aws.github.io/eks-charts
          helm repo add jetstack https://charts.jetstack.io
          helm repo add argo https://argoproj.github.io/argo-helm
          helm repo update

          VPC_ID=$(aws eks describe-cluster --name $(CLUSTER_NAME) --query "cluster.resourcesVpcConfig.vpcId" --output text)

          # Install AWS Load Balancer Controller
          helm upgrade --install aws-load-balancer-controller eks/aws-load-balancer-controller \
            -n $(HELM_NAMESPACE) \
            --set clusterName=$(CLUSTER_NAME) \
            --set region=$(AWS_REGION) \
            --set vpcId=$VPC_ID \
            --set serviceAccount.create=false \
            --set serviceAccount.name=$(SERVICE_ACCOUNT) \
            --wait --timeout 300s

          # Install Cert-Manager
          helm upgrade --install cert-manager jetstack/cert-manager \
            -n cert-manager --create-namespace \
            --set installCRDs=true

          # Install ArgoCD
          helm upgrade --install argocd argo/argo-cd \
            -n argocd --create-namespace

          echo "‚úÖ Platform setup completed successfully!"